{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a3dfc9",
   "metadata": {},
   "source": [
    "Name : Ng Chen Ting \n",
    "# Part B\n",
    "## Task 1. Processing Data Stream\n",
    "### Streaming Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1208862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import geohash2\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from bson import ObjectId\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "host_ip = \"192.168.1.14\"\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('Streaming Climate & Hotspot Data')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Set up the Spark DataFrame to read from Kafka topics\n",
    "kafka_sdf = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{host_ip}:9092\") \\\n",
    "    .option(\"subscribe\", \"Assignment_TaskB_Climate, Assignment_TaskB_Hotspots\") \\\n",
    "    .load()\n",
    "\n",
    "climate_sdf = kafka_sdf.select('value')\n",
    "\n",
    "# Open a mongoClient Connection\n",
    "mongo_client = MongoClient(\n",
    "                host=f'{host_ip}',\n",
    "                port=27017\n",
    "                )\n",
    "db = mongo_client['fit3182_assignment_db'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f7ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_10_minutes(time_1, time_2):\n",
    "    \"\"\"\n",
    "    A function to check if the two times are within 10 minutes of each other\n",
    "    \"\"\"\n",
    "    time_format = '%H:%M:%S'  \n",
    "    time1 = datetime.strptime(time_1, time_format)\n",
    "    time2 = datetime.strptime(time_2, time_format)\n",
    "    \n",
    "    # Calculate time difference\n",
    "    time_difference = abs((time2 - time1).total_seconds())\n",
    "    \n",
    "    # Return True if time difference is less than 10 mins\n",
    "    return time_difference <= 600       # 600 seconds is 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aff8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_batch(df, epoch_id):\n",
    "    \"\"\"\n",
    "    A function where it takes in and process a batch every 10 mins.\n",
    "    \"\"\"\n",
    "    print(\"Batch \" + str(epoch_id))\n",
    "    data = df.collect()\n",
    "    climate = {}\n",
    "    hotspots = []\n",
    "\n",
    "    # Decode each data (Climate and Hotspots from Terra and Aqua) in the batch\n",
    "    for row in data:\n",
    "        json_dict = json.loads(row.value.decode(\"utf-8\"))\n",
    "        \n",
    "        # Add Geohash Precision 3 and 5 values for all climate and hotspot data\n",
    "        latitude = json_dict.get('latitude')\n",
    "        longitude = json_dict.get('longitude')\n",
    "        geo_precision_3 = geohash2.encode(latitude, longitude, precision=3)\n",
    "        geo_precision_5 = geohash2.encode(latitude, longitude, precision=5)\n",
    "        json_dict['geohash3'] = geo_precision_3\n",
    "        json_dict['geohash5'] = geo_precision_5\n",
    "\n",
    "        # Append it to it's respective set/list according to producer_id\n",
    "        producer_id = json_dict.get('producer_id')\n",
    "        if producer_id == 'producer_climate':\n",
    "            climate = json_dict\n",
    "        elif producer_id == 'producer_hotspot_aqua':\n",
    "            hotspots.append(json_dict)\n",
    "        elif producer_id == 'producer_hotspot_terra':\n",
    "            hotspots.append(json_dict)\n",
    "   \n",
    "    # Only perform pre-processing for database preparation if climate data exists\n",
    "    # Else, do nothing for this batch\n",
    "    if climate != {}:\n",
    "        # Send data to respective handlers for pre-processing\n",
    "        hotspots = hotspots_manager(hotspots)\n",
    "        climate = climate_manager(climate, hotspots)\n",
    "\n",
    "        # Write to MongoDB\n",
    "        writeToMongoDB(climate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ed79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def climate_manager(climate, hotspots):\n",
    "    \"\"\"\n",
    "    A function to do pre-processing that prepares the climate data to be inserted into MongoDB\n",
    "    \"\"\"\n",
    "    # Initialise hotspots column \n",
    "    climate['hotspots'] = []\n",
    "\n",
    "    # Only perform pre-processing if there are hotspots\n",
    "    if hotspots is not None:\n",
    "        for hotspot in hotspots:\n",
    "            \n",
    "            # Check if climate & hotspot are nearby using Geohash precision 3\n",
    "            # Else, ignore the hotspot\n",
    "            if climate['geohash3'] == hotspot['geohash3']:\n",
    "                \n",
    "                # Determine cause of fire\n",
    "                if climate['air_temperature_celcius'] > 20 and climate['GHI_w/m2'] > 180:\n",
    "                    hotspot['cause'] = 'natural'\n",
    "                else:\n",
    "                    hotspot['cause'] = 'other'\n",
    "\n",
    "                # Append the hotspot under the climate's hotspot section\n",
    "                if 'hotspots' in climate:  \n",
    "                    climate['hotspots'].append(hotspot)\n",
    "                \n",
    "    return climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153c726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotspots_manager(hotspots):\n",
    "        \n",
    "    # Dictionary to store merged JSON objects\n",
    "    merged_objects = defaultdict(lambda: {'latitude': '', 'longitude': '', 'confidence': 0, 'surface_temperature_celcius': 0, 'producer_id': '', 'created_time': '', 'geohash3': '', 'geohash5': ''})\n",
    "\n",
    "    # Iterate over the JSON data\n",
    "    for item in hotspots:\n",
    "        \n",
    "        geohash5 = item['geohash5']\n",
    "        created_time = item['created_time']\n",
    "\n",
    "        # Check if there's already a record with the same geohash5\n",
    "        if geohash5 in merged_objects:\n",
    "            # Check if the created_time is within 10 minutes of any existing record\n",
    "            found_match = False\n",
    "            for existing_item in merged_objects[geohash5]:\n",
    "                existing_time = existing_item['created_time']\n",
    "                \n",
    "                if within_10_minutes(created_time, existing_time):\n",
    "                    # Calculate the new values and update/replace\n",
    "                    existing_item['surface_temperature_celcius'] = (existing_item['surface_temperature_celcius'] + int(item['surface_temperature_celcius'])) / 2\n",
    "                    existing_item['confidence'] = (existing_item['confidence'] + item['confidence']) / 2\n",
    "                    for key in ['latitude', 'longitude', 'producer_id', 'geohash3', 'geohash5']:\n",
    "                        if existing_item['created_time'] > created_time:\n",
    "                            existing_item[key] = item[key]\n",
    "                    found_match = True\n",
    "                    break\n",
    "\n",
    "            # If no match is found within 10 minutes, add this record as a new entry\n",
    "            if not found_match:\n",
    "                merged_objects[geohash5].append(item)\n",
    "        else:\n",
    "            # If no existing record with the same geohash5, add this record as a new entry\n",
    "            merged_objects[geohash5] = [item]\n",
    "\n",
    "    # Convert merged_objects dictionary to a list of merged JSON objects\n",
    "    merged_list = []\n",
    "    for key, value in merged_objects.items():\n",
    "        for item in value:\n",
    "            merged_list.append(item)\n",
    "\n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcca11e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToMongoDB(data):\n",
    "    \"\"\"\n",
    "    A function to prepare the document according the data model format and insert to MongoDB Database\n",
    "    \"\"\"\n",
    "    date_obj = datetime.strptime(data['date'], '%d/%m/%Y')\n",
    "    \n",
    "    document = {}\n",
    "    document['_id'] = ObjectId()\n",
    "    document['date'] = date_obj\n",
    "    document['station'] = 9999        # Self-set station number \n",
    "    document[\"air_temperature_celcius\"] = data['air_temperature_celcius']\n",
    "    document['relative_humidity'] = data['relative_humidity']\n",
    "    document['windspeed_knots'] = data['windspeed_knots']\n",
    "    document['max_wind_speed'] = data['max_wind_speed']\n",
    "    document['precipitation'] = data['precipitation']\n",
    "    document['GHI_w/m2'] = data['GHI_w/m2']\n",
    "    document['hotspots'] = []\n",
    "    \n",
    "    for each in data['hotspots']:\n",
    "        time_obj = datetime.strptime(each['created_time'], '%H:%M:%S')\n",
    "        combined_datetime = datetime(date_obj.year, date_obj.month, date_obj.day, time_obj.hour, time_obj.minute, time_obj.second)\n",
    "\n",
    "        hotspot = {}\n",
    "        hotspot['latitude'] = each['latitude']\n",
    "        hotspot['longitude'] = each['longitude']\n",
    "        hotspot['time'] = combined_datetime\n",
    "        hotspot['confidence'] = each['confidence']\n",
    "        hotspot['surface_temperature_celcius'] = each['surface_temperature_celcius']\n",
    "        hotspot['cause'] = each['cause']\n",
    "        document['hotspots'].append(hotspot)\n",
    "\n",
    "    print(document)\n",
    "\n",
    "    # Insert climate data into collection \n",
    "    db['Assignment_2_Climates'].insert_one(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b14ddd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 169\n",
      "{'_id': ObjectId('664f2a1b2a5f61900b329a10'), 'date': datetime.datetime(2024, 6, 17, 0, 0), 'station': 9999, 'air_temperature_celcius': 16, 'relative_humidity': 48.4, 'windspeed_knots': 8.1, 'max_wind_speed': 15.9, 'precipitation': ' 0.00G', 'GHI_w/m2': 139, 'hotspots': [{'latitude': -37.4368, 'longitude': 149.1191, 'time': datetime.datetime(2024, 6, 17, 8, 17, 7), 'confidence': 56, 'surface_temperature_celcius': 52, 'cause': 'other'}, {'latitude': -37.3493, 'longitude': 149.3691, 'time': datetime.datetime(2024, 6, 17, 19, 14, 34), 'confidence': 66, 'surface_temperature_celcius': 80, 'cause': 'other'}, {'latitude': -37.624, 'longitude': 149.314, 'time': datetime.datetime(2024, 6, 17, 0, 7, 26), 'confidence': 90, 'surface_temperature_celcius': 66, 'cause': 'other'}]}\n",
      "Batch 170\n",
      "{'_id': ObjectId('664f2a202a5f61900b329a11'), 'date': datetime.datetime(2024, 6, 18, 0, 0), 'station': 9999, 'air_temperature_celcius': 21, 'relative_humidity': 58.7, 'windspeed_knots': 7.8, 'max_wind_speed': 13.0, 'precipitation': ' 0.00I', 'GHI_w/m2': 167, 'hotspots': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopping query.\n"
     ]
    }
   ],
   "source": [
    "writer = (\n",
    "    climate_sdf.writeStream\n",
    "    .option(\"checkpointLocation\", \"./climate_sdf_checkpoints\")\n",
    "    .outputMode('append')\n",
    "    .trigger(processingTime='10 seconds') # Process data every 10 secs\n",
    "    .foreachBatch(process_data_batch)\n",
    ")\n",
    "\n",
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "finally:\n",
    "    mongo_client.close()\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d86b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
